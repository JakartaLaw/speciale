{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# found in \n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/reinforce/reinforce_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from agents import PolicyGradientAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class PolicyGradientAgent(object):\n",
    "    def __init__(self, alpha, gamma=0.99, n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = gamma\n",
    "        self.lr = alpha\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "\n",
    "        self.model_file = fname\n",
    "\n",
    "    def build_policy_network(self):\n",
    "        _input = Input(shape=(self.input_dims,))\n",
    "        advantages = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(_input)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "\n",
    "            return K.sum(-log_lik*advantages)\n",
    "\n",
    "        policy = Model([_input, advantages], outputs=[probs])\n",
    "\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "\n",
    "        predict = Model([_input], outputs=[probs])\n",
    "\n",
    "        return policy, predict\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        actions = np.zeros([len(action_memory), self.n_actions])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G - mean) / std\n",
    "\n",
    "        cost = self.policy.train_on_batch([state_memory, self.G], actions)\n",
    "\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: 4.00  average score 4.00\n",
      "episode:  1 score: 47.00  average score 25.50\n",
      "episode:  2 score: 7.00  average score 19.33\n",
      "episode:  3 score: -2.00  average score 14.00\n",
      "episode:  4 score: 1.00  average score 11.40\n",
      "episode:  5 score: 22.00  average score 13.17\n",
      "episode:  6 score: 1.00  average score 11.43\n",
      "episode:  7 score: 4.00  average score 10.50\n",
      "episode:  8 score: -1.00  average score 9.22\n",
      "episode:  9 score: 6.00  average score 8.90\n",
      "episode:  10 score: -2.00  average score 7.91\n",
      "episode:  11 score: 20.00  average score 8.92\n",
      "episode:  12 score: -2.00  average score 8.08\n",
      "episode:  13 score: -1.00  average score 7.43\n",
      "episode:  14 score: 14.00  average score 7.87\n",
      "episode:  15 score: 4.00  average score 7.62\n",
      "episode:  16 score: 11.00  average score 7.82\n",
      "episode:  17 score: -1.00  average score 7.33\n",
      "episode:  18 score: 0.00  average score 6.95\n",
      "episode:  19 score: 12.00  average score 7.20\n",
      "episode:  20 score: 1.00  average score 6.90\n",
      "episode:  21 score: 18.00  average score 7.41\n",
      "episode:  22 score: 6.00  average score 7.35\n",
      "episode:  23 score: 17.00  average score 7.75\n",
      "episode:  24 score: 8.00  average score 7.76\n",
      "episode:  25 score: 0.00  average score 7.46\n",
      "episode:  26 score: 8.00  average score 7.48\n",
      "episode:  27 score: 8.00  average score 7.50\n",
      "episode:  28 score: 5.00  average score 7.41\n",
      "episode:  29 score: 12.00  average score 7.57\n",
      "episode:  30 score: 3.00  average score 7.42\n",
      "episode:  31 score: 5.00  average score 7.34\n",
      "episode:  32 score: 1.00  average score 7.15\n",
      "episode:  33 score: 0.00  average score 6.94\n",
      "episode:  34 score: 25.00  average score 7.46\n",
      "episode:  35 score: 5.00  average score 7.39\n",
      "episode:  36 score: 16.00  average score 7.62\n",
      "episode:  37 score: 11.00  average score 7.71\n",
      "episode:  38 score: -2.00  average score 7.46\n",
      "episode:  39 score: 6.00  average score 7.42\n",
      "episode:  40 score: 4.00  average score 7.34\n",
      "episode:  41 score: 18.00  average score 7.60\n",
      "episode:  42 score: 5.00  average score 7.53\n",
      "episode:  43 score: 3.00  average score 7.43\n",
      "episode:  44 score: -1.00  average score 7.24\n",
      "episode:  45 score: 6.00  average score 7.22\n",
      "episode:  46 score: 3.00  average score 7.13\n",
      "episode:  47 score: 11.00  average score 7.21\n",
      "episode:  48 score: 3.00  average score 7.12\n",
      "episode:  49 score: 1.00  average score 7.00\n",
      "episode:  50 score: 10.00  average score 7.06\n",
      "episode:  51 score: 37.00  average score 7.63\n",
      "episode:  52 score: 5.00  average score 7.58\n",
      "episode:  53 score: 10.00  average score 7.63\n",
      "episode:  54 score: -1.00  average score 7.47\n",
      "episode:  55 score: 1.00  average score 7.36\n",
      "episode:  56 score: 2.00  average score 7.26\n",
      "episode:  57 score: 3.00  average score 7.19\n",
      "episode:  58 score: 4.00  average score 7.14\n",
      "episode:  59 score: 2.00  average score 7.05\n",
      "episode:  60 score: 3.00  average score 6.98\n",
      "episode:  61 score: 5.00  average score 6.95\n",
      "episode:  62 score: 1.00  average score 6.86\n",
      "episode:  63 score: 15.00  average score 6.98\n",
      "episode:  64 score: 10.00  average score 7.03\n",
      "episode:  65 score: 9.00  average score 7.06\n",
      "episode:  66 score: 7.00  average score 7.06\n",
      "episode:  67 score: -1.00  average score 6.94\n",
      "episode:  68 score: 14.00  average score 7.04\n",
      "episode:  69 score: 4.00  average score 7.00\n",
      "episode:  70 score: 10.00  average score 7.04\n",
      "episode:  71 score: 11.00  average score 7.10\n",
      "episode:  72 score: 4.00  average score 7.05\n",
      "episode:  73 score: 11.00  average score 7.11\n",
      "episode:  74 score: 11.00  average score 7.16\n",
      "episode:  75 score: 35.00  average score 7.53\n",
      "episode:  76 score: 0.00  average score 7.43\n",
      "episode:  77 score: 2.00  average score 7.36\n",
      "episode:  78 score: 6.00  average score 7.34\n",
      "episode:  79 score: 9.00  average score 7.36\n",
      "episode:  80 score: 7.00  average score 7.36\n",
      "episode:  81 score: 1.00  average score 7.28\n",
      "episode:  82 score: 12.00  average score 7.34\n",
      "episode:  83 score: 18.00  average score 7.46\n",
      "episode:  84 score: 9.00  average score 7.48\n",
      "episode:  85 score: 29.00  average score 7.73\n",
      "episode:  86 score: 10.00  average score 7.76\n",
      "episode:  87 score: -2.00  average score 7.65\n",
      "episode:  88 score: 4.00  average score 7.61\n",
      "episode:  89 score: 2.00  average score 7.54\n",
      "episode:  90 score: 14.00  average score 7.62\n",
      "episode:  91 score: 8.00  average score 7.62\n",
      "episode:  92 score: 4.00  average score 7.58\n",
      "episode:  93 score: 7.00  average score 7.57\n",
      "episode:  94 score: -1.00  average score 7.48\n",
      "episode:  95 score: 3.00  average score 7.44\n",
      "episode:  96 score: 6.00  average score 7.42\n",
      "episode:  97 score: 0.00  average score 7.35\n",
      "episode:  98 score: 0.00  average score 7.27\n",
      "episode:  99 score: 3.00  average score 7.23\n",
      "episode:  100 score: 22.00  average score 7.38\n",
      "episode:  101 score: -1.00  average score 7.33\n",
      "episode:  102 score: 4.00  average score 6.90\n",
      "episode:  103 score: 2.00  average score 6.85\n",
      "episode:  104 score: 14.00  average score 7.01\n",
      "episode:  105 score: 4.00  average score 7.04\n",
      "episode:  106 score: 1.00  average score 6.83\n",
      "episode:  107 score: 8.00  average score 6.90\n",
      "episode:  108 score: 2.00  average score 6.88\n",
      "episode:  109 score: 12.00  average score 7.01\n",
      "episode:  110 score: 3.00  average score 6.98\n",
      "episode:  111 score: 10.00  average score 7.10\n",
      "episode:  112 score: 3.00  average score 6.93\n",
      "episode:  113 score: 28.00  average score 7.23\n",
      "episode:  114 score: 7.00  average score 7.31\n",
      "episode:  115 score: 4.00  average score 7.21\n",
      "episode:  116 score: 2.00  average score 7.19\n",
      "episode:  117 score: 16.00  average score 7.24\n",
      "episode:  118 score: 1.00  average score 7.26\n",
      "episode:  119 score: -2.00  average score 7.24\n",
      "episode:  120 score: 5.00  average score 7.17\n",
      "episode:  121 score: 5.00  average score 7.21\n",
      "episode:  122 score: 11.00  average score 7.14\n",
      "episode:  123 score: 1.00  average score 7.09\n",
      "episode:  124 score: 0.00  average score 6.92\n",
      "episode:  125 score: 3.00  average score 6.87\n",
      "episode:  126 score: 1.00  average score 6.88\n",
      "episode:  127 score: 15.00  average score 6.95\n",
      "episode:  128 score: 13.00  average score 7.00\n",
      "episode:  129 score: 19.00  average score 7.14\n",
      "episode:  130 score: 10.00  average score 7.12\n",
      "episode:  131 score: 10.00  average score 7.19\n",
      "episode:  132 score: 5.00  average score 7.19\n",
      "episode:  133 score: 6.00  average score 7.24\n",
      "episode:  134 score: 5.00  average score 7.29\n",
      "episode:  135 score: -2.00  average score 7.02\n",
      "episode:  136 score: 2.00  average score 6.99\n",
      "episode:  137 score: -1.00  average score 6.82\n",
      "episode:  138 score: 1.00  average score 6.72\n",
      "episode:  139 score: -1.00  average score 6.73\n",
      "episode:  140 score: 0.00  average score 6.67\n",
      "episode:  141 score: 12.00  average score 6.75\n",
      "episode:  142 score: 3.00  average score 6.60\n",
      "episode:  143 score: 7.00  average score 6.62\n",
      "episode:  144 score: 2.00  average score 6.61\n",
      "episode:  145 score: 21.00  average score 6.83\n",
      "episode:  146 score: 32.00  average score 7.09\n",
      "episode:  147 score: 3.00  average score 7.09\n",
      "episode:  148 score: 1.00  average score 6.99\n",
      "episode:  149 score: 49.00  average score 7.45\n",
      "episode:  150 score: 3.00  average score 7.47\n",
      "episode:  151 score: 3.00  average score 7.40\n",
      "episode:  152 score: 36.00  average score 7.39\n",
      "episode:  153 score: 2.00  average score 7.36\n",
      "episode:  154 score: 17.00  average score 7.43\n",
      "episode:  155 score: 25.00  average score 7.68\n",
      "episode:  156 score: 9.00  average score 7.76\n",
      "episode:  157 score: 4.00  average score 7.78\n",
      "episode:  158 score: 25.00  average score 8.00\n",
      "episode:  159 score: 15.00  average score 8.11\n",
      "episode:  160 score: 3.00  average score 8.12\n",
      "episode:  161 score: 1.00  average score 8.10\n",
      "episode:  162 score: 10.00  average score 8.15\n",
      "episode:  163 score: 5.00  average score 8.19\n",
      "episode:  164 score: 9.00  average score 8.13\n",
      "episode:  165 score: 0.00  average score 8.03\n",
      "episode:  166 score: 2.00  average score 7.96\n",
      "episode:  167 score: 2.00  average score 7.91\n",
      "episode:  168 score: 3.00  average score 7.95\n",
      "episode:  169 score: 3.00  average score 7.84\n",
      "episode:  170 score: 13.00  average score 7.93\n",
      "episode:  171 score: 10.00  average score 7.93\n",
      "episode:  172 score: 5.00  average score 7.87\n",
      "episode:  173 score: 0.00  average score 7.83\n",
      "episode:  174 score: 17.00  average score 7.89\n",
      "episode:  175 score: -1.00  average score 7.77\n",
      "episode:  176 score: 14.00  average score 7.56\n",
      "episode:  177 score: -1.00  average score 7.55\n",
      "episode:  178 score: 18.00  average score 7.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  179 score: 8.00  average score 7.73\n",
      "episode:  180 score: 1.00  average score 7.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4861b00d91af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6757cacd088b>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# for keras the CUDA commands must come before importing the keras libraries\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "pg_agent = PolicyGradientAgent(alpha=0.0005, gamma=0.99, n_actions=action_size, \n",
    "                               input_dims=state_size)\n",
    "n_games = 500\n",
    "pg_scores = []\n",
    "\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    for time in range(500):\n",
    "        action = pg_agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        score += reward\n",
    "        pg_agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        #ddqn_agent.learn()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    pg_scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(pg_scores[max(0, i-100):(i+1)])\n",
    "    print('episode: ', i,'score: %.2f' % score,\n",
    "          ' average score %.2f' % avg_score)\n",
    "\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        #pg_agent.save_model()\n",
    "\n",
    "        pass\n",
    "    \n",
    "x = [i+1 for i in range(n_games)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3d5565cc2550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "Model(inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = Input(shape=(8,))\n",
    "dense1 = Dense(64, activation='relu')(_input)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "probs = Dense(4, activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x130eded10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model([_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (master thesis)",
   "language": "python",
   "name": "masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
