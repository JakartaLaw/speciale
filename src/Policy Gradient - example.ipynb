{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# found in \n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/reinforce/reinforce_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from agents import PolicyGradientAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class PolicyGradientAgent(object):\n",
    "    def __init__(self, alpha, gamma=0.99, n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = gamma\n",
    "        self.lr = alpha\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "\n",
    "        self.model_file = fname\n",
    "\n",
    "    def build_policy_network(self):\n",
    "        _input = Input(shape=(self.input_dims,))\n",
    "        advantages = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(_input)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "\n",
    "            return K.sum(-log_lik*advantages)\n",
    "\n",
    "        policy = Model([_input, advantages], outputs=[probs])\n",
    "\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "\n",
    "        predict = Model([_input], outputs=[probs])\n",
    "\n",
    "        return policy, predict\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        actions = np.zeros([len(action_memory), self.n_actions])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G - mean) / std\n",
    "\n",
    "        cost = self.policy.train_on_batch([state_memory, self.G], actions)\n",
    "\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: 4.00  average score 4.00\n",
      "episode:  1 score: 47.00  average score 25.50\n",
      "episode:  2 score: 7.00  average score 19.33\n",
      "episode:  3 score: -2.00  average score 14.00\n",
      "episode:  4 score: 1.00  average score 11.40\n",
      "episode:  5 score: 22.00  average score 13.17\n",
      "episode:  6 score: 1.00  average score 11.43\n",
      "episode:  7 score: 4.00  average score 10.50\n",
      "episode:  8 score: -1.00  average score 9.22\n",
      "episode:  9 score: 6.00  average score 8.90\n",
      "episode:  10 score: -2.00  average score 7.91\n",
      "episode:  11 score: 20.00  average score 8.92\n",
      "episode:  12 score: -2.00  average score 8.08\n",
      "episode:  13 score: -1.00  average score 7.43\n",
      "episode:  14 score: 14.00  average score 7.87\n",
      "episode:  15 score: 4.00  average score 7.62\n",
      "episode:  16 score: 11.00  average score 7.82\n",
      "episode:  17 score: -1.00  average score 7.33\n",
      "episode:  18 score: 0.00  average score 6.95\n",
      "episode:  19 score: 12.00  average score 7.20\n",
      "episode:  20 score: 1.00  average score 6.90\n",
      "episode:  21 score: 18.00  average score 7.41\n",
      "episode:  22 score: 6.00  average score 7.35\n",
      "episode:  23 score: 17.00  average score 7.75\n",
      "episode:  24 score: 8.00  average score 7.76\n",
      "episode:  25 score: 0.00  average score 7.46\n",
      "episode:  26 score: 8.00  average score 7.48\n",
      "episode:  27 score: 8.00  average score 7.50\n",
      "episode:  28 score: 5.00  average score 7.41\n",
      "episode:  29 score: 12.00  average score 7.57\n",
      "episode:  30 score: 3.00  average score 7.42\n",
      "episode:  31 score: 5.00  average score 7.34\n",
      "episode:  32 score: 1.00  average score 7.15\n",
      "episode:  33 score: 0.00  average score 6.94\n",
      "episode:  34 score: 25.00  average score 7.46\n",
      "episode:  35 score: 5.00  average score 7.39\n",
      "episode:  36 score: 16.00  average score 7.62\n",
      "episode:  37 score: 11.00  average score 7.71\n",
      "episode:  38 score: -2.00  average score 7.46\n",
      "episode:  39 score: 6.00  average score 7.42\n",
      "episode:  40 score: 4.00  average score 7.34\n",
      "episode:  41 score: 18.00  average score 7.60\n",
      "episode:  42 score: 5.00  average score 7.53\n",
      "episode:  43 score: 3.00  average score 7.43\n",
      "episode:  44 score: -1.00  average score 7.24\n",
      "episode:  45 score: 6.00  average score 7.22\n",
      "episode:  46 score: 3.00  average score 7.13\n",
      "episode:  47 score: 11.00  average score 7.21\n",
      "episode:  48 score: 3.00  average score 7.12\n",
      "episode:  49 score: 1.00  average score 7.00\n",
      "episode:  50 score: 10.00  average score 7.06\n",
      "episode:  51 score: 37.00  average score 7.63\n",
      "episode:  52 score: 5.00  average score 7.58\n",
      "episode:  53 score: 10.00  average score 7.63\n",
      "episode:  54 score: -1.00  average score 7.47\n",
      "episode:  55 score: 1.00  average score 7.36\n",
      "episode:  56 score: 2.00  average score 7.26\n",
      "episode:  57 score: 3.00  average score 7.19\n",
      "episode:  58 score: 4.00  average score 7.14\n",
      "episode:  59 score: 2.00  average score 7.05\n",
      "episode:  60 score: 3.00  average score 6.98\n",
      "episode:  61 score: 5.00  average score 6.95\n",
      "episode:  62 score: 1.00  average score 6.86\n",
      "episode:  63 score: 15.00  average score 6.98\n",
      "episode:  64 score: 10.00  average score 7.03\n",
      "episode:  65 score: 9.00  average score 7.06\n",
      "episode:  66 score: 7.00  average score 7.06\n",
      "episode:  67 score: -1.00  average score 6.94\n",
      "episode:  68 score: 14.00  average score 7.04\n",
      "episode:  69 score: 4.00  average score 7.00\n",
      "episode:  70 score: 10.00  average score 7.04\n",
      "episode:  71 score: 11.00  average score 7.10\n",
      "episode:  72 score: 4.00  average score 7.05\n",
      "episode:  73 score: 11.00  average score 7.11\n",
      "episode:  74 score: 11.00  average score 7.16\n",
      "episode:  75 score: 35.00  average score 7.53\n",
      "episode:  76 score: 0.00  average score 7.43\n",
      "episode:  77 score: 2.00  average score 7.36\n",
      "episode:  78 score: 6.00  average score 7.34\n",
      "episode:  79 score: 9.00  average score 7.36\n",
      "episode:  80 score: 7.00  average score 7.36\n",
      "episode:  81 score: 1.00  average score 7.28\n",
      "episode:  82 score: 12.00  average score 7.34\n",
      "episode:  83 score: 18.00  average score 7.46\n",
      "episode:  84 score: 9.00  average score 7.48\n",
      "episode:  85 score: 29.00  average score 7.73\n",
      "episode:  86 score: 10.00  average score 7.76\n",
      "episode:  87 score: -2.00  average score 7.65\n",
      "episode:  88 score: 4.00  average score 7.61\n",
      "episode:  89 score: 2.00  average score 7.54\n",
      "episode:  90 score: 14.00  average score 7.62\n",
      "episode:  91 score: 8.00  average score 7.62\n",
      "episode:  92 score: 4.00  average score 7.58\n",
      "episode:  93 score: 7.00  average score 7.57\n",
      "episode:  94 score: -1.00  average score 7.48\n",
      "episode:  95 score: 3.00  average score 7.44\n",
      "episode:  96 score: 6.00  average score 7.42\n",
      "episode:  97 score: 0.00  average score 7.35\n",
      "episode:  98 score: 0.00  average score 7.27\n",
      "episode:  99 score: 3.00  average score 7.23\n",
      "episode:  100 score: 22.00  average score 7.38\n",
      "episode:  101 score: -1.00  average score 7.33\n",
      "episode:  102 score: 4.00  average score 6.90\n",
      "episode:  103 score: 2.00  average score 6.85\n",
      "episode:  104 score: 14.00  average score 7.01\n",
      "episode:  105 score: 4.00  average score 7.04\n",
      "episode:  106 score: 1.00  average score 6.83\n",
      "episode:  107 score: 8.00  average score 6.90\n",
      "episode:  108 score: 2.00  average score 6.88\n",
      "episode:  109 score: 12.00  average score 7.01\n",
      "episode:  110 score: 3.00  average score 6.98\n",
      "episode:  111 score: 10.00  average score 7.10\n",
      "episode:  112 score: 3.00  average score 6.93\n",
      "episode:  113 score: 28.00  average score 7.23\n",
      "episode:  114 score: 7.00  average score 7.31\n",
      "episode:  115 score: 4.00  average score 7.21\n",
      "episode:  116 score: 2.00  average score 7.19\n",
      "episode:  117 score: 16.00  average score 7.24\n",
      "episode:  118 score: 1.00  average score 7.26\n",
      "episode:  119 score: -2.00  average score 7.24\n",
      "episode:  120 score: 5.00  average score 7.17\n",
      "episode:  121 score: 5.00  average score 7.21\n",
      "episode:  122 score: 11.00  average score 7.14\n",
      "episode:  123 score: 1.00  average score 7.09\n",
      "episode:  124 score: 0.00  average score 6.92\n",
      "episode:  125 score: 3.00  average score 6.87\n",
      "episode:  126 score: 1.00  average score 6.88\n",
      "episode:  127 score: 15.00  average score 6.95\n",
      "episode:  128 score: 13.00  average score 7.00\n",
      "episode:  129 score: 19.00  average score 7.14\n",
      "episode:  130 score: 10.00  average score 7.12\n",
      "episode:  131 score: 10.00  average score 7.19\n",
      "episode:  132 score: 5.00  average score 7.19\n",
      "episode:  133 score: 6.00  average score 7.24\n",
      "episode:  134 score: 5.00  average score 7.29\n",
      "episode:  135 score: -2.00  average score 7.02\n",
      "episode:  136 score: 2.00  average score 6.99\n",
      "episode:  137 score: -1.00  average score 6.82\n",
      "episode:  138 score: 1.00  average score 6.72\n",
      "episode:  139 score: -1.00  average score 6.73\n",
      "episode:  140 score: 0.00  average score 6.67\n",
      "episode:  141 score: 12.00  average score 6.75\n",
      "episode:  142 score: 3.00  average score 6.60\n",
      "episode:  143 score: 7.00  average score 6.62\n",
      "episode:  144 score: 2.00  average score 6.61\n",
      "episode:  145 score: 21.00  average score 6.83\n",
      "episode:  146 score: 32.00  average score 7.09\n",
      "episode:  147 score: 3.00  average score 7.09\n",
      "episode:  148 score: 1.00  average score 6.99\n",
      "episode:  149 score: 49.00  average score 7.45\n",
      "episode:  150 score: 3.00  average score 7.47\n",
      "episode:  151 score: 3.00  average score 7.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# for keras the CUDA commands must come before importing the keras libraries\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "pg_agent = PolicyGradientAgent(alpha=0.0005, gamma=0.99, n_actions=action_size, \n",
    "                               input_dims=state_size)\n",
    "n_games = 500\n",
    "pg_scores = []\n",
    "\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    for time in range(500):\n",
    "        action = pg_agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        score += reward\n",
    "        pg_agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        #ddqn_agent.learn()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    pg_scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(pg_scores[max(0, i-100):(i+1)])\n",
    "    print('episode: ', i,'score: %.2f' % score,\n",
    "          ' average score %.2f' % avg_score)\n",
    "\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        #pg_agent.save_model()\n",
    "\n",
    "        pass\n",
    "    \n",
    "x = [i+1 for i in range(n_games)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3d5565cc2550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "Model(inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = Input(shape=(8,))\n",
    "dense1 = Dense(64, activation='relu')(_input)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "probs = Dense(4, activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x130eded10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model([_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (master thesis)",
   "language": "python",
   "name": "masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
