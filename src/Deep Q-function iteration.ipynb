{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling colab problem\n",
    "import os\n",
    "if not 'data' in os.listdir('..'):\n",
    "    print('needs data folder. imports through git')\n",
    "    !git clone https://github.com/JakartaLaw/speciale.git\n",
    "    print(os.listdir())\n",
    "    os.chdir('speciale//src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import EnvironmentModel1 as Environment\n",
    "from environments import translate_action_model1 as translate_action\n",
    "from environments import reward_scaler_model1 as reward_scaler\n",
    "from environments.model1 import scale_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MemoryBuffer():\n",
    "    \n",
    "    def __init__(self, action_size, state_size, mem_size=1000000, batch_size=50000):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_cntr = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.action_size = action_size\n",
    "        self.state_size =state_size\n",
    "        \n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, state_size))\n",
    "        self.q_memory = np.zeros((self.mem_size, action_size))\n",
    "        \n",
    "    def store_transition(self, state, q_values):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        self.state_memory[index] = state\n",
    "        self.q_memory[index] = q_values\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample_batch(self):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        _batch_size = min(self.mem_cntr, self.batch_size)\n",
    "        batch = np.random.choice(max_mem, _batch_size)\n",
    "        print(_batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        q_values = self.q_memory[batch]\n",
    "        return states, q_values\n",
    "\n",
    "class QFunctionScaler():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.q_mean = None\n",
    "        self.q_std = None\n",
    "\n",
    "    def fit(self, q_vals):\n",
    "        self.q_mean = np.mean(q_vals, axis=0)\n",
    "        self.q_std = np.std(q_vals, axis=0)\n",
    "\n",
    "    def transform(self, q_vals):\n",
    "        q_vals_ = (q_vals - self.q_mean ) /self.q_std\n",
    "        return q_vals_\n",
    "\n",
    "    def inverse_transform(self, q_vals):\n",
    "        q_vals_ = (q_vals * self.q_std ) + self.q_std\n",
    "        return q_vals_\n",
    "\n",
    "def build_network(input_dims, output_dims, fc1_size, fc2_size, lr):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(fc1_size, activation='relu', input_dim=input_dims))\n",
    "    model.add(Dense(fc2_size, activation='relu'))\n",
    "    model.add(Dense(output_dims, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=lr))\n",
    "    return model\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, alpha, gamma, layer1_size, layer2_size, statespace_dims, action_dims):\n",
    "        self.lr = alpha\n",
    "        self.gamma = gamma\n",
    "        self.fc1_size = layer1_size\n",
    "        self.fc2_size = layer2_size\n",
    "        self.statespace_dims = statespace_dims\n",
    "        self.action_dims = action_dims\n",
    "        self.q_network = build_network(self.statespace_dims, self.action_dims, \n",
    "                                       self.fc1_size, self.fc2_size, self.lr) \n",
    "        \n",
    "        self.memory_buffer = MemoryBuffer(action_dims, statespace_dims)\n",
    "        self.q_scaler = QFunctionScaler()\n",
    "    \n",
    "    def learn(self, verbose=True, rebuild_network=True):\n",
    "        states, q_vals = self.memory_buffer.sample_batch()\n",
    "\n",
    "        self.q_scaler.fit(q_vals)\n",
    "        q_vals = self.q_scaler.transform(q_vals)\n",
    "\n",
    "        if rebuild_network:\n",
    "            print('rebuilding network')\n",
    "            self.q_network = build_network(self.statespace_dims, self.action_dims, \n",
    "                                       self.fc1_size, self.fc2_size, self.lr)\n",
    "        \n",
    "        history_ = self.q_network.fit(states, q_vals, epochs=150, validation_split=0.3, \n",
    "                                    callbacks=[EarlyStopping(patience=5)], verbose=0, batch_size=512)\n",
    "        \n",
    "        \n",
    "        n_epochs = history_.epoch[-1] \n",
    "        loss_, val_loss_ = history_.history['loss'][-1], history_.history['val_loss'][-1]\n",
    "        scaled_val_loss_ = np.float(np.mean(self.q_scaler.q_std) * val_loss_)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'# epochs: {n_epochs}, loss: {loss_:.3}, val loss: {val_loss_:.3}, scaled val loss: {scaled_val_loss_:.3}')\n",
    "            \n",
    "    def remember(self, state, q_values):\n",
    "        self.memory_buffer.store_transition(state, q_values)\n",
    "        \n",
    "    def act(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        q_values = self.q_network.predict(state)\n",
    "        action, q_value = np.argmax(q_values), self.q_scaler.inverse_transform(q_values)\n",
    "        return action, q_value\n",
    "        \n",
    "    def predict_q_vals(self, new_states, rewards, done):\n",
    "        rewards = rewards[np.newaxis, :]\n",
    "        \n",
    "        if done:\n",
    "            q_vals = rewards\n",
    "        else:\n",
    "            q_vals_next = self.q_network.predict(new_states)\n",
    "            q_vals_next = self.q_scaler.inverse_transform(q_vals_next)\n",
    "            q_vals_next_star = np.max(q_vals_next, axis=1)\n",
    "            #q_vals_next_star = np.max(q_vals_next, axis=0)\n",
    "\n",
    "            q_vals = rewards + self.gamma * q_vals_next_star\n",
    "\n",
    "\n",
    "        return q_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateSpaceSampler():\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.Q_min = 60\n",
    "        self.Q_max = 60\n",
    "        \n",
    "        self.K_min = 0\n",
    "        self.K_max = 5\n",
    "        \n",
    "        self.G_min = 0.0\n",
    "        self.G_max = 5.0\n",
    "        \n",
    "        self.Z_min = -200.0\n",
    "        self.Z_max = 200.0\n",
    "        \n",
    "        self.beta_K_min = 0.2\n",
    "        self.beta_K_max = 6.0\n",
    "        \n",
    "        self.beta_L_min = 0.2\n",
    "        self.beta_L_max = 6.0\n",
    "        \n",
    "    def draw_state(self):\n",
    "        Q = np.random.randint(self.Q_min, self.Q_max + 1)\n",
    "        K = np.random.randint(self.K_min, self.K_max + 1)\n",
    "        G = np.random.uniform(self.G_min, self.G_max)\n",
    "        Z = np.random.uniform(self.Z_min, self.Z_max)\n",
    "        \n",
    "        beta_K = np.random.uniform(self.beta_K_min, self.beta_K_max)\n",
    "        beta_L = np.random.uniform(self.beta_L_min, self.beta_L_max)\n",
    "        \n",
    "        return (Q, G, K, Z, beta_K, beta_L)\n",
    "        \n",
    "    def expand_pos_states(self):\n",
    "        \"\"\"creates to expand possible states\"\"\"\n",
    "        self.Q_min = self.Q_min - 1\n",
    "        self.Q_max = self.Q_max - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_state(states):\n",
    "    \"\"\"helper for turning states in to parameters\"\"\"\n",
    "    states_ = states[0:4]\n",
    "    parameters = {\n",
    "        'beta_K': states[4],\n",
    "        'beta_L': states[5]\n",
    "    }\n",
    "    return states_, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'beta_K' : 1,\n",
    "    'beta_L' : 1,\n",
    "    'sigma_epsilon' : 0.1, \n",
    "    'S_min': 120.0,\n",
    "    'alpha': 4.609,\n",
    "    'eta_G': 0.164,\n",
    "    'eta_G_sq' : 0.015,\n",
    "    'delta': 0.209,\n",
    "    'sigma_epsilon': 15.11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(0.005, 0.99 ,16, 8, 6, 4)\n",
    "sss = StateSpaceSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "# epochs: 22, loss: 0.000222, val loss: 0.000245\n"
     ]
    }
   ],
   "source": [
    "ACTIONS = [0, 25, 37, 45]\n",
    "\n",
    "rewards_all = list()\n",
    "while sss.Q_min > 24:\n",
    "    print(sss.Q_min)\n",
    "    for i in range(1000):\n",
    "\n",
    "        # ugly\n",
    "        _states_ = sss.draw_state()\n",
    "        _states, _params = transform_state(_states_)\n",
    "        scaled_state = scale_states(*_states_)\n",
    "\n",
    "        # ugly\n",
    "        rewards = np.zeros(shape=4)\n",
    "        new_states = np.zeros(shape=(4, 6)) \n",
    "        for ix, action in enumerate(ACTIONS):\n",
    "            env.reset(_states, _params)\n",
    "\n",
    "            new_scaled_state, reward, done, _info = env.step(action)\n",
    "            #scaled_reward = reward_scaler(reward, env.beta_K, env.beta_L)\n",
    "            #rewards[ix] = scaled_reward\n",
    "            rewards[ix] = reward\n",
    "            new_states[ix] = new_scaled_state\n",
    "        \n",
    "        rewards_all.append(np.argmax(rewards))\n",
    "        q_vals = agent.predict_q_vals(new_states, rewards, done)\n",
    "        agent.remember(scaled_state, q_vals)\n",
    "\n",
    "    agent.learn()\n",
    "    sss.expand_pos_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-d0ac36fdedd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0m_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-92d3103c673a>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m           model, mode)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       callbacks = cbks.configure_callbacks(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m     if (context.executing_eagerly()\n\u001b[1;32m    417\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    592\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    616\u001b[0m           gen_dataset_ops.anonymous_iterator_v2(\n\u001b[1;32m    617\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m               output_shapes=self._flat_output_shapes))\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speciale-wQK9LG7Y/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36manonymous_iterator_v2\u001b[0;34m(output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AnonymousIteratorV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         output_shapes)\n\u001b[0m\u001b[1;32m    127\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_AnonymousIteratorV2Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards_history, action_history = list(), list()\n",
    "EPISODES = 150\n",
    "agent.epsilon=0.0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    env.reset()\n",
    "    state = env.states\n",
    "    for time in range(18, 500):\n",
    "        action, q_vals = agent.act(state)\n",
    "        _action = translate_action(action)\n",
    "        next_state, reward, done, _ = env.step(_action)\n",
    "        scaled_reward = (reward - 24 ) / 5\n",
    "\n",
    "        \n",
    "        rewards_history.append([reward, e, time])\n",
    "        action_history.append([_action, e, time])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            #print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, np.mean(rewards_history), agent.epsilon))\n",
    "            break\n",
    "\n",
    "df_DQAgent = pd.DataFrame(rewards_history, columns=['rewards', 'episode', 'Q'])\n",
    "df_DQAgent_action = pd.DataFrame(action_history, columns=['actions', 'episode', 'Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action():\n",
    "    act = np.random.randint(0, 4)\n",
    "    return [0, 25, 37, 45][act]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_history, action_history = list(), list()\n",
    "EPISODES = 150\n",
    "for e in range(EPISODES):\n",
    "    env.reset()\n",
    "    state = env.states\n",
    "    for time in range(18, 500):\n",
    "        _action = random_action()\n",
    "        next_state, reward, done, _ = env.step(_action)\n",
    "\n",
    "        \n",
    "        rewards_history.append([reward, e, time])\n",
    "        action_history.append([_action, e, time])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "df_random_agent = pd.DataFrame(rewards_history, columns=['rewards', 'episode', 'Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_history, action_history = list(), list()\n",
    "EPISODES = 150\n",
    "for e in range(EPISODES):\n",
    "    env.reset()\n",
    "    state = env.states\n",
    "    for time in range(18, 500):\n",
    "        _action = 0\n",
    "        next_state, reward, done, _ = env.step(_action)\n",
    "\n",
    "        \n",
    "        rewards_history.append([reward, e, time])\n",
    "        action_history.append([_action, e, time])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "df_0_agent = pd.DataFrame(rewards_history, columns=['rewards', 'episode', 'Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_history, action_history = list(), list()\n",
    "EPISODES = 150\n",
    "for e in range(EPISODES):\n",
    "    env.reset()\n",
    "    state = env.states\n",
    "    for time in range(18, 500):\n",
    "        _action = 37\n",
    "        next_state, reward, done, _ = env.step(_action)\n",
    "\n",
    "        \n",
    "        rewards_history.append([reward, e, time])\n",
    "        action_history.append([_action, e, time])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "df_37_agent = pd.DataFrame(rewards_history, columns=['rewards', 'episode', 'Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_history, action_history = list(), list()\n",
    "EPISODES = 150\n",
    "for e in range(EPISODES):\n",
    "    env.reset()\n",
    "    state = env.states\n",
    "    for time in range(18, 500):\n",
    "        _action = 45\n",
    "        next_state, reward, done, _ = env.step(_action)\n",
    "\n",
    "        \n",
    "        rewards_history.append([reward, e, time])\n",
    "        action_history.append([_action, e, time])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "df_45_agent = pd.DataFrame(rewards_history, columns=['rewards', 'episode', 'Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(df_random_agent.drop('episode', axis=1).groupby('Q').mean(), label='Random Agent')\n",
    "plt.plot(df_DQAgent.drop('episode', axis=1).groupby('Q').mean(), label='DQ Agent')\n",
    "plt.plot(df_37_agent.drop('episode', axis=1).groupby('Q').mean(), label='37 hour Agent')\n",
    "plt.plot(df_45_agent.drop('episode', axis=1).groupby('Q').mean(), label='45 hour Agent')\n",
    "plt.plot(df_0_agent.drop('episode', axis=1).groupby('Q').mean(), label='0 hour Agent')\n",
    "plt.plot(df_random_agent.drop('episode', axis=1).groupby('Q').mean(), label='random Agent')\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = df_DQAgent_action.drop('episode', axis=1).groupby('Q').mean() \\\n",
    "              + df_DQAgent_action.drop('episode', axis=1).groupby('Q').std()\n",
    "lower_bound = df_DQAgent_action.drop('episode', axis=1).groupby('Q').mean() \\\n",
    "              - df_DQAgent_action.drop('episode', axis=1).groupby('Q').std()\n",
    "\n",
    "plt.plot(df_DQAgent_action.drop('episode', axis=1).groupby('Q').mean())\n",
    "plt.fill_between(range(18,61), upper_bound['actions'], lower_bound['actions'], alpha=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (master thesis)",
   "language": "python",
   "name": "masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
