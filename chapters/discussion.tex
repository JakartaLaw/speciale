\section{Discussion and Further Perspectives}

Due to the time-horizon of a master thesis, certain decisions has been made in the interest of time. Had more time been available, some decisions might have been done different. I calibrated the parameters using an agent based approach. Considering I assumed that the parameters driving the Mincer equation was the same for men and women, this was a reasonable decision, but considering the scope of the paper, It could have been interesting had I estimated those using the entire model. In that case those parameters could not have been estimated using method of simulated moments, because of relying on the interquartile range to capture the idiosyncratic wage path. Instead, I would have had to rely on indirect inference, a method closely related to method of simulated moments, but with the caveat that one can use alternative measures, e.g., quartiles.

This paper has used aggregate, public available data for calibration and parameter estimation. Should this paper have used actual data, considerably more time would have gone into cleaning and curating the final data sets. Furthermore, it would also require I had had access to more detailed data from Statistics Denmark, which I do not currently have. Had more detailed been available, the empirical moments could have been used in the optimization problem, namely a better weight matrix for the method of moments estimation could have been constructed. This would also make it possible to compare the marginal distribution of states and actions, making inference of the model easier.

This paper has focused exclusively on temporal difference learning, namely deep Q-learning. I believe that reinforcement learning could end up being an integral part of the econometrician's toolbox, and I therefore believe other methods should be researched. Especially Actor Critic method (which is a Monte Carlo method) could potentially yield interesting results. This is due to the fact, that one is constrained to a discrete action space using Q-learning, whereas actor-critic methods allow for continuous action spaces. Monte Carlo tree search should also be investigated further, due to the fact these have been key in the Alpha Go and Alpha Go Zero to solve the board game Go, Chess and Shogi \parencite{silver_general_2018}. Finally, I would speculate, that since data usually governs economists choice of models, a more ``data centric'' approach could develop within the field of economics. More specifically, I would suspect, research should be done on how to estimate agents value function, directly from data, without the need to construct a model of the environment. This might allow for doing counter factual analysis on utility maximizing agents, without the need of explicitly define the agents' utility functions.

Finally, it should be addressed, that deep reinforcement learning does not guarantee the policy converges to the optimal policy. This is true, but still one should consider these methods have beat humans in games where rationality was instrumental, e.g., Chess, Go and Star Craft. So when economists assume perfectly rational agents, I believe, this should be considered a useful assumption for modelling, not an accurate depiction of real humans, in essence making the critic somewhat invalid. On the other hand, economists also use agent based modelling. These methods assume agents make choices based on some heuristics. The methods used in this paper, do to a certain extent sit between the two extremes, rational agents and agent based modelling. The agents actively search for the optimal policy, which might very well lead to something mimicking heuristics. Summing up, I believe the framework for thinking about reinforcement learning agents in the broader context of economics, will need time to mature.
