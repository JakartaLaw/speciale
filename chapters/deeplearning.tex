\section{Deep Learning}



\subsection{Why machine learning}

Deep learning is a subset of machine learning or sometimes referred to as statistical learning. Machine learning methods try to model either the joint or the marginal distribution of some covariates and som target. In statistics a model is usually explicitly formulated. A typical example could be linear regression. The covariates is decided on, and tweaked such that they fit the model. The linear regression will yield a parameter vector which can then be interpreted. And this interpretation is usually the focus - parameter inference. Example: Does an increase in the minimum salary have a negative effect on BNP pr. capita. Machine Learning focuses on prediction. That is, we want to predict some target conditional on some covariates. And the specific model is not necessarily important. Instead a focus on Out-Of-Sample error is the focus. These predictive methods lend themselves well where causal inference is not needed. Example: What is the expected consumption on a monthly basis by person with a given characteristics. When formulating a traditional econometric method, f.x. OLS, there is standard ways to infer if the model is well specified. In machine learning, in general the same asymptotic results regarding regularization of the model is not possible. Instead usually sample splitting will be used. Take a data set, split the data set into two partitions a test set and a training set. The final algorithm is only used on the test data set once, when the hyper parameters of the machine learning algorithm has been tuned. Machine learning methods, as mentioned before usually has associated hyper parameters. These are parameters of the model, which is not fitted by training the data, rather these are specifications of the algorithm before doing training the model. Much of machine learning is about finding the right hyper parameters and regularizing the model in an intelligent way. (XXX Elements of statistical learning). Now why is this paper concerned with ML methods. This is due to the fact, that when estimating the value function, or the policy function, one cannot be sure that this follows a linear function. In fact these value functions and policy functions might be highly non-linear, which is where machine learning methods shine. Furthermore what is modelled trying to capture the value function is a conditional expectation.

\begin{equation}
    \E[Y \mid X=x]
\end{equation}

which can be directly translated into the expected value function conditional on a given state. And in this case the causal interpretation of one states influence on the value function is not of interest, rather it's the accuracy of the expected value function\footnote{Obviously, intelligent agents usually do causal inference on there actions. They might not do a certain action exactly because they have some causal notion of how the environment will evolve conditional on their action.}. Since deep neural networks has been the usual way to implement reinforcement algorithms, other machine learning methods can also be used.

\subsection{Deep Neural Networks}

Deep learning (feed forward networks) which is used in this paper is in fact just layered non-linear functions. They can be illustrated by figure 



These functions could f.x.  be sigmoid, tanh or rectified linear units. Using the chain-rule the deep learning algorithm can be used for numerical optimization using a gradient descent algorithm. The Objective function in this case is the loss-function i.e. the function that measures how well the algorithm perform.

\begin{equation}
    \frac{\partial}{\partial\theta} \Loss = \frac{\partial}{\partial\theta} \lp \sum \ell_i \rp = \frac{\partial}{\partial\theta} \lp \sum \lp \hat{Y}_i  - Y_i \rp^2 \rp = \frac{\partial}{\partial\theta} \lp \sum \lp f^{\theta}(X_i)  - Y_i \rp^2 \rp
\end{equation}

\subsection{Historical overview}

\subsection{Theoretical considerations}




