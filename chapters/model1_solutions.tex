\section{Soution Methods}\label{sec:solution_methods}

\subsection{Value Function Iteration}

Three different solution methods is presented in this section. I lead with (a slightly modified) value function iteration, next deep Q-Learning is presented ending with double deep Q-learning iteration.

The value function iteration presented in this paper has certain modifications to the algorithm presented in \ref{sec:dynamic_programming}. First and foremost, I consider the Q-function instead of the value function allowing to store state-action value pairs. Second it should be noted that in this formulation some states are continuous not allowing for tabular solutions. Thirdly, the dimension of the state-space + the size of the grid made it infeasible\footnote{In my initial attempt to solve the model by value function iteration, I attempted to get the expectation of the value function using Gauss-Hermite integration, and discretizing the state space. The solution time was infeasible, which was why my approach changed.} to solve the model by classical ways of solving a model by discretizing the state space in a grid. 

The model is solved using backward induction. This is due to the fact the model terminates in a deterministic fashion when the agents reach a certain age. For each step a large random sample of states is drawn conditioning on a given age. For each of the states the agent takes each of the possible actions storing the results. This way a large sample of rewards and states can be generated. Furthermore for each action taken (if the state is not terminal) the  Q-function can be evaluated in the new state, taken the max of each possible action, allowing for the estimation of the value function. A graphical representation of the concept is shown in figure \ref{fig:vfi_figure}. 

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.15]{figures/vfi_figure.png}
    \caption{Value Function Iteration}
    \label{fig:vfi_figure}
\end{figure}

The Q-function for each action in the action space is made by mapping each $a\in \actionspace$:

\begin{equation}
 Q(S_t, a) = R_{t+1} + \gamma \underset{a'\in \actionspace}{\max} Q(S_{t+1}, a')
\end{equation}

In that sense this can be considered akin to Monte Carlo integration, however instead of approximating the expectation in a single point, rather find the distribution of the rewards + discounted value function over the entire state space. The idea is to approximate the integral by using a statistical method, in this case deep learning even though another machine learning method would be equally good. Consider $f$ to be a deep neural network, that has the property:

\begin{equation}
    f: \statespace \mapsto\R^{\mid \actionspace \mid}
\end{equation}

For a given point in state space a prediction of the value function is computed for each possible action. This implies the method only is feasible for discrete state space. By trying to reduce the mean squared error between the true values of the Q-function, and the prediction, the $\E[Q(a, s)]$ can be found, which corresponds to integration as could be done using Gauss Hermite or Monte Carlo integration. The algorithm is presented in algorithm \ref{alg:dqi}.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Estimated Q function}
 Initialize $\tilde{Age} = Age_{max}$\;
 Initialize empty lists for storing results: $X, Y$\;
 Initialize memory counter $j=1$\;
 \While{$\tilde{Age} > Age_{min}$}{
  Draw $\{s_{i}\}_{i=1}^{N}$, where $s_i \sim Uniform(\statespace) \mid Age=\tilde{Age}$ \;
  \ForEach{$s_{i}$}{
  Create empty array $Z$ of length $\mid \actionspace \mid$\;
  \eIf{$\tilde{Age}= Age_{max}$}{
   \ForEach{$a_k \in \actionspace$}{
    $Z[k] \leftarrow R_{t+1}, \quad  R_{t+1}\sim \mathcal{E}\mid A_t = a_k, S_t = s_{i}$ \;
   }
   }{
   \ForEach{$a_k \in \actionspace$}{
    $Z[k] \leftarrow R_{t+1} + \gamma \underset{a \in \actionspace}{\max} \hat{Q}(S_{t+1}, a), \quad  R_{t+1}, S_{t+1} \sim \mathcal{E} \mid A_t = a_k, S_t = s_{i} $\;
    }
  }
  $Y[j] \leftarrow Z, X[j] \leftarrow s_i$\;
  $j = j + 1$\;
  }
  Estimate $\hat{Q}$ by training a Deep NN using samples from $X, Y$\;
  Decrease $\tilde{Age}$ be one\;
 }
 \caption{Value Function Iteration Solution Method}
 \label{alg:dqi}
 \end{algorithm}

Since I use a deep neural network to approximate the $Q$-function, the architecture and hyper parameters of the network needs to be considered. The same is true for the sampling scheme.

For each age 20.000 random samples is drawn. This is because any smaller number of draws seemed to be detrimental to the performance. This is inline with standard deep learning practices, where neural networks are known to be very data hungry. A random sample of 100.000 observations is used when training the network. If I have not yet accumulated 100.000 observations the algorithm draws all observations. The architecture of network is fairly simple being a two-layer fully connected network. First layer being 16 nodes wide, second fully connected layer being 8 nodes wide. I found that mini batching, did not seem to work well on this particular task, and instead I train on all observations, using a validation split of 30 \%, training for a maximum of 150 epochs\footnote{A epoch corresponds to a full sweep through the the data set.} and finally I allow for early stopping, that is, when the validation loss is not further decreasing, I stop the training of the network. I do allow the algorithm a patience of 5, Implying that the algorithm will try to lower its validation loss for five additional epochs before terminating the training.


\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqi_model1_beta_2_solution_benchmark_paths.png}
  \caption{Simulated Paths}
  \label{fig:dqi_solution_beta2_path}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqi_model1_beta_2_solution_benchmark_variance.png}
  \caption{Variance of Paths}
  \label{fig:dqi_solution_beta2_var}
\end{subfigure}
    \caption{Value Function Iteration Solution vs. Benchmark $(\beta_L = 2)$}
    \label{fig:dqi_solution_beta2}
\end{figure}

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqi_model1_beta_4_solution_benchmark_paths.png}
  \caption{Simulated Paths}
  \label{fig:dqi_solution_beta4_path}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqi_model1_beta_4_solution_benchmark_variance.png}
  \caption{Variance of Paths}
  \label{fig:dqi_solution_beta4_var}
\end{subfigure}
    \caption{Value Function Iteration Solution vs. Benchmark $(\beta_L = 4)$}
    \label{fig:dqi_solution_beta4}
\end{figure}

Figure \ref{fig:dqi_solution_beta2} and \ref{fig:dqi_solution_beta4} shows the results of the Value Function Iteration algorithm\footnote{The plots use the name DQIteration (Deep Q-Function Iteration) instead of Value Function Iteration.} compared to 4 benchmarks. Three deterministic agents working either $0$, $37$ or $45$ hours pr. week and one agent taking random actions. Figure \ref{fig:dqi_solution_beta2} shows the utility for each step over the life cycle when the preference for leisure is fixed at $\beta_L = 2$. As the figure shows the VFI agent learns to navigate the environment, just as well as the best deterministic agent. Looking to the RHS plot of \ref{fig:dqi_solution_beta2} it is clear that there is a substantial overlap between the two agents. The variance of the path is represented as one standard deviation of the utility for a given age for all episodes of the given agent. Figure \ref{fig:dqi_solution_beta4} compares the benchmark agents with the VFI solution when considering a preference for leisure $\beta_L = 4$. Again the VFI solution is as good as the best benchmark.

\subsection{Deep Q-learning}

In this paper I implement\footnote{Originally I used my own implementation, however slow performance, caused me to use and modify an existing implementation allowing for a speed up of a factor of 10. The same is true for the Double Deep Q-learning algorithm. The original implementation can be found at \textcite{tabor_deep_2020}.} the deep Q-learning algorithm used by \textcite{mnih_playing_2013} for beating Atari games as a way to solve model. A few modifications is made to the original implementation, due to the fact, the environment they were navigating only returned sensory data (an RGB representation of an image).  A part of their achievement was to transform these images into features that the value function accurately could map into scores of the game. Another difference is that this paper implements a scaling module of the variables for better performance.

Just as described in section \ref{sec:rl_theory} the algorithm tries to maximize the Bellman equation. However now the value-function is estimated using Deep neural network as a function approximator. Mathematically this can be described as finding:

\begin{equation}
    Q^*(S_t,A_t) = \E [R_{t+1} + \underset{a}{\max}  Q^*(S_{t+1}, a) \mid S_t, A_t]
\end{equation}

However here $Q^*(S, A)$ is approximated by a parametric function (in this case a Deep Neural Network) $Q(S, A ; \theta)$. Following the terminology of \textcite{mnih_playing_2013}, this function approximator is referred to as the Q-network. The Q-network can be trained using stochastic gradient descent as described in section \ref{sec:deep_learning}:

\begin{equation}
    \Loss_i(\theta_i) = \E \lsp (Y_i - Q(S_t, A_t ; \theta_i))^2 \rsp
\end{equation}

\begin{equation}
    Y_i = \E [R_{t+1} + \gamma \underset{a}\max Q(S_{t+1}, a; \theta_{i-1}) \mid S_t, A_t ]
\end{equation}

where $i$ implies the iteration of the algorithm, such $i$ increments by one for each update of the parameters $\theta$. The equation used for updating the weight of the neural network is described below:

\begin{equation}
    \nabla_{\theta_i} \Loss_i (\theta_i)  \E \lsp \lp Y_i - Q(S_t, A_t ; \theta) \rp \nabla_{\theta_i} Q(S_t, A_t; \theta_i) \rsp
\end{equation}

Following the formulation of \textcite{mnih_playing_2013} $Q(S_{t+1}, A_{t+1}; \theta_{i-1})$ is held fixed, allowing for just writing $Y_i$, and not the parametric form of $Y_i$.

A traditional choice would be to update the weight after each step in the algorithm only using the last sample. This would the correspond to the traditional Q-learning algorithm. This was the approach used by the TD Gammon agent created by Tesauro\footnote{I have not been able to get access to the original paper, so I have relied on the description made by Sutton and Barto.} as described by \textcite{sutton_reinforcement_2018}. The approach of using Q-learning with a non-linear function approximator has been shown to diverge under certain circumstances, and did not extend itself well to learning any other game than backgammon \parencite{tsitsiklis_analysis_1997}. To accommodate this problem a replay buffer is implemented. At each step an entry is made to the replay memory containing $(s_t, a_t, r_t, s_{t+1})$. This data set $\mathcal{D}$ has a capacity of $N$ entries. Using the replay memory a random mini batch is sampled used to update the weights of the Q-network. Note here that the capacity of the memory buffer should be greater, by a substantial margin than the number of samples drawn. The random sample has a couple of advantages. It decorrelates the observations used to update the weights, allowing for better training \textcite{mnih_playing_2013}. Using a replay buffer requires off-policy learning which is the reason for using Q-learning. This is due to the fact, that current parameters $\theta$ is not the same as those generating the data. Note, that experiences is drawn randomly, and no experiences (which could have important insights) is prioritized. The full algorithm is summarized in algorithm \ref{alg:dqlearning}.

\begin{algorithm}[H]
\SetAlgoLined
 Initialize replay memory $\mathcal{D}$ with capacity $N$\;
 Initialize action-value function $Q$ with random weights\;
 Initialize memory counter $j=1$\;
 \ForEach {episode $\in \{1, 2, \cdots M \}$}{
  Initialize sequence with an initial state $s_1$. This is drawn randomly.\;
    \For{$t \in \{1, 2, \cdots, T \}$}{
    With probability $\epsilon$ select a random action $a_t$\;
    Otherwise select $A_t = \underset{a}{\argmax}Q^*(S_t, a ; \theta)$\;
    Execute action $A_t$ in environment and observe reward $R_t$ and the new state $S_{t+1}$\;
    Store transition $(S_t, A_t, R_{t+1}, S_{t+1})$ in $\mathcal{D}$\;
    Sample random mini-batch of transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ from $\mathcal{D}$\;
    Set $
      Y_j = \begin{cases}
        R_j & \text{if terminal states} \\
        R_j + \gamma \underset{a}{\max}Q(S_{t+1}, a; \theta) & \text{if non-terminal states}
      \end{cases}
    $ \;
    Perform gradient descent step on $(Y_j - Q(S_t, A_t ; \theta))^2$ \;
    Increment $j$\;
  }
}
\caption{Deep Q-learning}
\label{alg:dqlearning}
\end{algorithm}

Following the implementation \textcite{mnih_playing_2013} a trick is applied to reduce the number of computations needed when running the algorithm. Ordinarily the Q-function maps a state action pair to a scalar value estimate of the value function. The most obvious implementation would let the action be part of the input to the function, however, such implementation would require $\mid \actionspace \mid$ number of look ups at each step. This is due to the fact that each action in the action space must be used for evaluation. Instead the Q-function in this implementation maps the state space to a scalar value for each action $Q: \statespace \mapsto \R^{\mid \actionspace \mid}$. 

The solution used the following hyper parameters and architecture decisions: The Deep Neural network consists of an input layer of same size as the state space, followed by two fully connected layers of width $256$ using rectified linear units as activation functions. Finally the network uses a linear output layer for its predictions. The output layer is of the same size as the action space. The learning rate, $\alpha=0.0005$ and I let epsilon be decremented after each update by: $\epsilon_i = \max (\epsilon_{i-1} \cdot 0.9999, 0.01$), where $0.01$ is the minimum exploration that will be done. The replay buffer has a capacity of 1 million rows, and the mini batches used for updating the parameters is $64$ rows. I use the Adam optimizer for the gradient descent step when updating the weights. I scale the state space so that each variable approximately has mean zero and standard deviation of 1 when doing the batch training. \textcite{goodfellow_deep_2016} argues that it increases performance, and in general accepted as being an important step for getting good performance out of neural networks. The impatience parameter is set to $\gamma=0.99$, using a standard value. I make the parameter $\beta_L$ a part of the state space, drawing a random value (uniformly) in the interval $[0.2, 6.0]$ at the beginning of each episode, letting the agent navigate through the environment with given preference for leisure. This is done, so only a single solution of the model is necessary for estimating the parameter later. I scale the rewards (so they are approximate zero mean and have a standard deviation of 1, conditional on the $\beta_L$ value. Again this is done to improve the training performance and to do introspection; it becomes possible to see if there is any trend in the training performance. The algorithm is trained for 3000 episodes. Figure \ref{fig:training_performance_simple_model}  (left plot) shows the training performance of the Deep Q-learning algorithm. The plot shows the agent's cumulative rewards over the life cycle of each episode. The performance begins at around 2.5 and ends at an average of $7.9$ as the asymptotic value. The performance seems to reach this asymptotic level at around 1000 episodes. 

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqn_training_performance_simple_model.png}
  \caption{DQN Training Performance}
  \label{fig:dqn_training_performance_simple}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/ddqn_training_performance_simple_model.png}
  \caption{Double DQN Training Performance}
  \label{fig:ddqn_training_performance_simple}
\end{subfigure}
    \caption{Comparing Training Perfance of Deep Q-learning and Double Deep Q-learning}
    \label{fig:training_performance_simple_model}
\end{figure}

Figure \ref{fig:dqn_solution_beta2} and \ref{fig:dqn_solution_beta4} compares the performance of the algorithm to two different benchmarks. One benchmark is the environment with $\beta_L = 2$ another with $\beta_L=4$. These differences in preferences will yield different optimal policies conditional on the $\beta_L$. I compare the policy chosen by the agent with different deterministic policies. Figure \ref{fig:dqn_solution_beta2} (left plot) shows either using $0, 37, 45$ hours pr. week as comparisons and an agent picking randomly. All agents have been simulated for 300 episodes, yielding the plots being averages, and the standard deviations are calculated on a pr. age basis for each agent. It should also be noted that $\epsilon$ (the exploration ratio of the agent) is set to 0, such that the agent now only greedily navigates the environment. Comparing this to the VFI agent I find  it reasonable to assume that the DQN agent has learned to navigate the environment. A small dip from around age 55 can be observed, but I believe this does not make the feat any less impressive, and furthermore I believe that the solution fairly accurately approximates the optimal policy. Again the best benchmark for $\beta_L = 2$ is an agent that works 45 hours a week. The deterministic and DQN agent seem to follow the same policy until  the last periods, where they diverge a little. I still to conclude that the differences in performance is very small. Figure \ref{fig:dqn_solution_beta4} shows the same plot just for $\beta_L = 4$. Again the DQN agent fairly accurately approximates the optimal policy, with even less variance than observed in with $\beta_L = 2$. One thing to note is, that a small hump can be observed for the DQN agent at around age 28, where it has found a way to beat the deterministic policy. Again this might just be variance of the simulations. 


\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqn_model1_beta_2_solution_benchmark_paths.png}
  \caption{Simulated Paths}
  \label{fig:dqn_solution_beta2_path}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqn_model1_beta_2_solution_benchmark_variance.png}
  \caption{Variance of Paths}
  \label{fig:dqn_solution_beta2_var}
\end{subfigure}
    \caption{Deep Q-learning Solution vs. Benchmark $(\beta_L = 2)$}
    \label{fig:dqn_solution_beta2}
\end{figure}

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqn_model1_beta_4_solution_benchmark_paths.png}
  \caption{Simulated Paths}
  \label{fig:dqn_solution_beta4_path}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/dqn_model1_beta_4_solution_benchmark_variance.png}
  \caption{Variance of Paths}
  \label{fig:dqn_solution_beta4_var}
\end{subfigure}
    \caption{Deep Q-Learning Solution vs. Benchmark $(\beta_L = 4)$}
    \label{fig:dqn_solution_beta4}
\end{figure}


\subsection{Double Deep Q-learning}

Deep Q-learning is good starting point for a learning algorithm using non-linear function approximation, however certain properties of the algorithm is problematic. \textcite{van_hasselt_deep_2015} argues this is mainly due to the fact that Deep Q-learning can tend to be overoptimistic in a problematic way. In general there exists two ways in which overoptimism can be good or at least not detrimental to performance. 1) If the algorithm uniformly overestimates the Q-function for all actions, then this is not associated with any problems, since taking the max of the Q-function w.r.t. to the action, will lead to the same result had the estimations been correct. In other words, performance would not change, however introspection of the algorithm might be harder. 2) It can be a good thing for an algorithm to be optimistic when faced with uncertainty. If an action would lead to observing an unexplored part of the state space, it might be a good thing to be optimistic in regard to exploration, allowing for possible new policies with higher yielding returns over the episode. Deep Q-learning do not conform to any of these properties, instead it usually overestimates the value of the action it has taken due to the max operator in the value estimation: $R_{t+1} + \gamma \underset{a}{\argmax} Q(S_{t+1},a)$. A simple extension presented by \textcite{van_hasselt_deep_2015} is to decouple the prediction with the evaluation. This can be done by having two different sets of weights, where one $\theta$ is used for the policy (policy weights) and one $\tilde{\theta}$ is used for the evaluation (target weights). This can be presented the following way. The Q-network target follows the equation below:

\begin{equation}\label{eq:dqn_target}
    Y_t^{Q} = R_{t+1} + \gamma Q(S_{t+1}, \underset{a}{\argmax} (S_{t+1}, a ; \theta) ; \theta)
\end{equation}

The Double DQN target is contrasts the DQN-target shown above, by using two sets of weights:

\begin{equation}\label{eq:ddqn_target}
    Y_t^{DoubleQ} = R_{t+1} + \gamma Q(S_{t+1}, \underset{a}{\argmax}Q(S_{t+1}, a; \theta); \tilde{\theta}) 
\end{equation}

This alleviates the problem of overestimating the values returned by the algorithm for the following reason. Consider \eqref{eq:dqn_target}, when calculating the value of the state action pair, one uses the same function to choose the action, prompting  a real risk of overestimation of the state action pair of the chosen action. The equation below \eqref{eq:ddqn_target} instead estimates the value of the position by using a different set of parameters, $\tilde{\theta}$. Following the implementation of \textcite{van_hasselt_deep_2015} instead of training 2 separate networks the target weights are inherited from the evaluation weights. This is to reduce time of training the algorithm, even though the policy and target network can not be considered perfectly decoupled. The implication of this design choice is $\tilde{\theta} = \theta^{previous}$, such that, the target estimation is made by the old weights, while the value of greedy policy uses the new weights:

\begin{equation}\label{eq:ddqn_target_final}
    Y_t^{DoubleQ} = R_{t+1} + \gamma Q(S_{t+1}, \underset{a}{\argmax}Q(S_{t+1}, a; \theta_t); \theta_t^{previous}) 
\end{equation}

The full algorithm, which to large extend mirrors the algorithm for Deep Q-learning is presented in algorithm \ref{alg:ddqlearning}:

\begin{algorithm}[H]
\SetAlgoLined
 Initialize replay memory $\mathcal{D}$ with capacity $N$\;
 Initialize action-value function $Q$ with random weights: $\theta, \tilde{\theta}$\;
 Initialize memory counter $j \leftarrow 1$\;
 Initialize $k$ when to replace target weights $\tilde{\theta}$\;
 \ForEach {episode $\in \{1, 2, \cdots M \}$}{
  Initialize sequence with an initial state $s_1$. This is drawn randomly.\;
    \For{$t \in \{1, 2, \cdots, T$ \}}{
    With probability $\epsilon$ select a random action $a_t$\;
    Otherwise select $A_t = \underset{a}{\argmax}Q^*(S_t, A_t ; \theta)$\;
    Execute action $A_t$ in environment and observe reward $R_t$ and the new state $S_{t+1}$\;
    Store transition $(S_t, A_t, R_{t+1}, S_{t+1})$ in $\mathcal{D}$\;
    Sample random mini-batch of transitions $(S_t, A_t, A_{t+1}, S_{t+1})$ from $\mathcal{D}$\;
    Set $
      Y_j = \begin{cases}
        R_{t+1} & \text{if terminal states} \\
        R_{t+1} + \gamma Q(S_{t+1}, \underset{a}{\argmax} Q(S_t, a; \theta_t); \tilde{\theta_t}) & \text{if non-terminal states}
      \end{cases}
    $ \;
    Perform gradient descent step on $(Y_j - Q(S_t, A_t ; \theta))^2$ w.r.t $\theta$ \;
    If $j$ is divisible by $k$ replace target weights $\tilde{\theta} \leftarrow \theta$\;
    Increment $j$ \;
  }
}
\caption{Double Deep Q-learning}
\label{alg:ddqlearning}
\end{algorithm}

Just as with the Deep Q-learning implementation the Q-function map from: $\statespace \mapsto \R^{\mid \actionspace \mid}$, to reduce the number of computations. 

For the Double DQN agent I in general use the same hyper parameters used for the DQN agent: The network architecture is again an input layer of the size of the state space, followed by two fully connected layers with Rectified Linear units as activation functions ending with an output layer with linear activation of same size as the action space. Again I let the learning rate be $\alpha = 0.0005$ and the $\epsilon_i = \max (\epsilon_{i-1} \cdot 0.9999, 0.01)$, such that exploration is performed throughout the training period. Again the replay buffer has a capacity of a million rows and I use mini batches of size 64 to perform stochastic gradient descent on the weight of the neural network approximating the Q-function. I transform the states such that they are mean zero and a standard deviation of 1. The same is true for the rewards conditional on the $\beta_L$ value. I uniformly draw $\beta_L$ from the interval $[0.2, 6]$ and sets this to be part of the environment in the beginning of each episode. I use the Adam optimizer when updating the weights using gradient descent. I let the target network inherit the old weights every 100'th iteration of the algorithm. The algorithm is trained for 3000 episodes. Figure \ref{fig:training_performance_simple_model} (right plot) shows the performance. Very similar to the DQN, I find the performance stabilizes at around 1000 episodes, and reaches an asymptotic performance of $8.22$, which is slightly higher compared to the DQN agent which had an asymptotic value of $7.92$.

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/ddqn_model1_beta_2_solution_benchmark_paths.png}
  \caption{Simulated Paths}
  \label{fig:ddqn_solution_beta2_path}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/ddqn_model1_beta_2_solution_benchmark_variance.png}
  \caption{Variance of Paths}
  \label{fig:ddqn_solution_beta2_var}
\end{subfigure}
    \caption{Double Deep Q-Learning solution vs. benchmark $(\beta_L = 2)$}
    \label{fig:ddqn_solution_beta2}
\end{figure}

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/ddqn_model1_beta_4_solution_benchmark_paths.png}
  \caption{Simulated Paths}
  \label{fig:ddqn_solution_beta4_path}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/ddqn_model1_beta_4_solution_benchmark_variance.png}
  \caption{Variance of Paths}
  \label{fig:ddqn_solution_beta4_var}
\end{subfigure}
    \caption{Double Deep Q-Learning solution vs. benchmark $(\beta_L = 4)$}
    \label{fig:ddqn_solution_beta4}
\end{figure}

Figure \ref{fig:ddqn_solution_beta2} and \ref{fig:ddqn_solution_beta4} shows the performance of the Double DQN agent comparing it to two different preferences of leisure: $\beta_L = 2$ and $\beta_L = 4$. Four agents is used for comparison: An agent that chooses actions randomly, and three agents that in an deterministic fashion works $0, 37$ or $45$ hours a week. The results of the Double DQN agent is comparable to those found within the other agents (VFI and DQN). Figure \ref{fig:ddqn_solution_beta2} shows the performance where the preference for leisure $\beta_L = 2$. Again the best deterministic policy is the 45 hour agent, which is very close to what the DQN agent chooses, except for the last 5 years of the agent's life cycle where the utility is slightly lower. Comparing the variance of the utility on RHS plot of figure \ref{fig:ddqn_solution_beta2}, there is overlap of the simulations of the Double DQN agent and the 45-hours a week agent. Figure \ref{fig:ddqn_solution_beta4} compares the Double DQN agent to the agents when the preference for leisure $\beta_L = 4$. Again, the optimal policy pretty closely mirrors the best deterministic agent (the agent that does not work), with a clear overlap presented in figure on RHS.
