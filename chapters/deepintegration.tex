\section{Deep Q-Function Iteration}

Deep Q-function iteration lends extends the classic value function iteration. However, value function iteration as discussed does not fare well in high dimensional state space and furthermore it's very computational heavy.

The idea is to approximate the integral by using a statistical method, and in this case Deep Learning (another machine learning method would be equally good). Still implore a backwards induction. Consider $f$ to be a machine learning function, that maps from State Space into $\vert \actionspace \vert$. I.e. For a given point in state space a prediction of the value function is computed for each possible action. This implies the method only is feasible for discrete state space. By trying to reduce the mean squared error between the the true values of the Q-function, and the prediction, the $\E[Q(a, s)]$ can be found, which corresponds to integration as could be done using f.x. Gauss Hermite or Monte Carlo integration.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Write here the result }
 Initialize $\tilde{Age} = Age_{max}$\;
 Initialize empty lists for storing results: $X, Y$\;
 Initialize memory counter $j=1$\;
 \While{$\tilde{Age} > Age_{min}$}{
  Draw $\{s_i\}_{i=1}^{i=N}$, where $s_i \sim \statespace \mid Age=\tilde{Age}$ \;
  \ForEach{$s_i$}{
  Create empty array $Z$ of length $\mid \actionspace \mid$\;
  \eIf{$\tilde{Age}= Age_{max}$}{
   \ForEach{$a_k \in \actionspace$}{
    $Z[k] \leftarrow r(s_i, a_k)$ \;
   }
   }{
   \ForEach{$a_k \in \actionspace$}{
    $Z[k] \leftarrow r(s_i, a_k)$ + $\gamma \underset{a' \in \actionspace}{\max}\lsp \hat{q}(a', s_i') \rsp$\;
    }
  }
  $Y[j] \leftarrow Z, X[j] \leftarrow s_i$\;
  $j = j + 1$\;
  }
  Estimate $\hat{Q}$ by training a Deep NN using samples from $X, Y$.
 }
 \caption{Deep Q-function iteration solution method}
 
Since i use a deep neural network to approximate the $Q$-function, certain things need to be considered. First and foremost I need to consider the architecture of the network. Next I need to consider the train

For each age i draw 20.000 random samples. This is because any smaller number of draw seemed to be detrimental to the performance. This is inline with standard Deep Learning practices. These kinds of network is known to be very data hungry. When training the network i draw a random sample of 50.000 observations. If I have not yet accumulated 50.000 observations the algorithm draws all observations. The architecture of network is fairly simple being a two-layer fully connected network. First layer being 16 nodes wide, second fully connected layer being 8 nodes wide. I found that mini batching, did not seem to work well on this particular task, and instead i train on all observations, using a validation split of 30 \%, training for a maximum of 150 epochs and finally i allow for early stopping, that is, when the validation loss is not furthering decreasing i stop the training of the network. I do allow the algorithm a patience of 5. Implying that the algorithm will try to lower it's validation loss for five additional epochs before terminating the training.

\end{algorithm}

